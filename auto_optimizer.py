import subprocess
import itertools
import sys
import argparse

# final parameters
PARAM_GRID = {
    'baseline': {
        'mlp_epochs': [300]
    },
    'gnn_only': {
        'gnn_dim': [16],
        'gnn_epochs': [500],
        'gnn_lr': [0.005]
    },
    'proposed': {
        'gnn_dim': [16],
        'gnn_epochs': [500],
        'mlp_epochs': [300]
    }
}

def run_command(cmd):
    """ì‰˜ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ê³  ì¶œë ¥ì„ ë°˜í™˜. ì—ëŸ¬ ì‹œ None ë°˜í™˜"""
    print(f"cmd> {cmd}")
    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    stdout, stderr = process.communicate()
    
    if process.returncode != 0:
        print(f"Error: {stderr}")
        return None # ì—ëŸ¬ ë°œìƒ ì‹œ None ë°˜í™˜
    return stdout

def parse_rmse(output):
    """train.py CV ê²°ê³¼ì—ì„œ RMSE íŒŒì‹±"""
    if output is None: return 999.0
    for line in output.split('\n'):
        if "CV Result" in line:
            return float(line.split("Average RMSE:")[1].strip())
    return 999.0

def run_process(task_mode):
    """
    task_mode:
      - 'opt': Grid Searchë¥¼ í†µí•´ ìµœì  íŒŒë¼ë¯¸í„° íƒìƒ‰ í›„ í•™ìŠµ/í‰ê°€
      - 'run': PARAM_GRIDì˜ ì²« ë²ˆì§¸ ê°’ì„ ìµœì ê°’ìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¦‰ì‹œ í•™ìŠµ/í‰ê°€
    """
    print(f"\nðŸš€ Starting Process in [{task_mode.upper()}] mode...")

    for mode, grid in PARAM_GRID.items():
        print(f"\n========== Target Model: [{mode}] ==========")
        
        best_score = 999.0
        best_params = {}

        # -------------------------------------------------------
        # MODE 1: Optimization (Grid Search + CV)
        # -------------------------------------------------------
        if task_mode == 'opt':
            print(f"ðŸ” [Opt] Searching for best parameters...")
            keys, values = zip(*grid.items())
            combinations = [dict(zip(keys, v)) for v in itertools.product(*values)]
            
            for params in combinations:
                cmd = f"python3 train.py --mode {mode} --job cv --k_fold 3"
                for k, v in params.items():
                    cmd += f" --{k} {v}"
                
                output = run_command(cmd)
                
                if output is None:
                    print(f"   Params: {params} -> Failed âŒ")
                    continue
                    
                rmse = parse_rmse(output)
                print(f"   Params: {params} -> RMSE: {rmse:.4f}")
                
                if rmse < best_score:
                    best_score = rmse
                    best_params = params
            
            if not best_params and combinations:
                print(f"âš ï¸ {mode} ëª¨ë“œì˜ ëª¨ë“  CV ì‹¤í–‰ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                continue
                
            print(f"ðŸ† Best Params Found: {best_params} (RMSE: {best_score:.4f})")

        # -------------------------------------------------------
        # MODE 2: Direct Run (Skip CV)
        # -------------------------------------------------------
        else: # task_mode == 'run'
            # ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ê°’ë§Œ ê°€ì ¸ì™€ì„œ ì„¤ì •
            best_params = {k: v[0] for k, v in grid.items()}
            print(f"â© [Run] Skipping optimization. Using config: {best_params}")

        # -------------------------------------------------------
        # Common Step: Final Training & Evaluation
        # -------------------------------------------------------
        print(f"ðŸ”¥ [Train] Starting final training with best params...")
        model_name = f"best_{mode}"
        
        train_cmd = f"python3 train.py --mode {mode} --job train --model_name {model_name}"
        for k, v in best_params.items():
            train_cmd += f" --{k} {v}"
            
        train_out = run_command(train_cmd)
        if train_out is None:
            print(f"âŒ Final Training Failed for {mode}")
            continue 
        
        print(f"ðŸ“Š [Eval] Starting evaluation...")
        eval_cmd = f"python3 evaluate.py --mode {mode} --model_name {model_name}"
        eval_out = run_command(eval_cmd)
        
        if eval_out:
            print(f"-------- Evaluation Result ({mode}) --------")
            print(eval_out)
            print("--------------------------------------------")
        else:
            print(f"âŒ Evaluation Failed for {mode}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    # 'opt' ë˜ëŠ” 'run' ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ë„ë¡ ì„¤ì • (ê¸°ë³¸ê°’: run)
    parser.add_argument('--task', type=str, choices=['opt', 'run'], default='run', 
                        help="opt: Grid Search ìˆ˜í–‰ / run: ì„¤ì •ëœ ê°’ìœ¼ë¡œ ì¦‰ì‹œ í•™ìŠµ")
    
    args = parser.parse_args()
    run_process(args.task)